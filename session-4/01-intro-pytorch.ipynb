{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch and deep-learning\n",
    "    Thomas Moreau <thomas.moreau@inria.fr>\n",
    "    Mathurin Massias <mathurin.massias@inria.fr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will get familiar with the deep learning framework `PyTorch`. A __deep learning framework__ is a library designed to build deep learning models __easily__, __quickly__ and __efficiently__. It usually contains modules (corresponding to different types of layers) that allow you to build models easily, an automatic differentiation machinery which computes gradients for you, and a series of optimization algorithms. As of today, there are various deep learning frameworks available, among which the most famous may be:\n",
    "\n",
    " * PyTorch\n",
    " * Keras-Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "\n",
    "[1. Defining a neural network in PyTorch](#basics)<br>\n",
    "- [1.1 Pytorch tutorials](#tutorials)<br>\n",
    "- [1.2 Definition using Sequential](#NNseq)<br>\n",
    "- [1.3 Definition using a custom class](#NNcustom)<br>\n",
    "- [1.4 Definition using a custom class without relying on PyTorch modules](#NNcustom+)<br>\n",
    "\n",
    "\n",
    "[2. Training a neural network in PyTorch](#NNtraining)<br>\n",
    "- [2.1  Loading openml data](#Loaddata)<br>\n",
    "- [2.2  The training loop](#TrainLoop)<br>\n",
    "\n",
    "[3. Using weights and biases](#wandb)<br>\n",
    "\n",
    "[4. Implementing a Resnet for tabular data](#resnet)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the following libraries and check that Pytorch is running on your computer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import openml\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basics'></a>\n",
    "# 1 - Defining a neural network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several possiblities to define a neural network architecture in PyTorch. In this part we will review three of them, from the simplest (and least flexible) to the most advanced (and most flexible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch documentation is clear and well written. Please use and abuse of it to better understand the various objects that we will use in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tutorials'></a>\n",
    "\n",
    "## 1.1 -Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses tensors to store the model inputs, outputs as well as model parameters. Tensors are just a data structure that ressembles numpy arrays or matrices, and their usage resemble that of numpy arrays. The main difference is that PyTorch keeps track of the computation perfomed to obtained a given tensor, in order to be able to perform back-propagation. For a review of basic operations on tensors (addition, multiplication, ...), please refer to the [PyTorch Blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) and notably the section on [tensors](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NNseq'></a>\n",
    "## 1.2 - Definition using Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Sequential` is the simplest way of defining a neural network in PyTorch. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(256, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation for `Sequential` is [here](https://pytorch.org/docs/master/generated/torch.nn.Sequential.html#torch.nn.Sequential)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1 - What kind of neural network is it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2 - What is the appropriate data input size for such a network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3 - Is it suited for a regression task, binary classification taks, multiclass classification task? What does `dim=1` means in the `Softmax`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4 - Count the number of parameters of this network. Check this programatically with `model.parameters()`.**\n",
    "\n",
    "_Solution :_ `solutions/01_pytorch_q04.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NNcustom'></a>\n",
    "## 1.3 - Definition using a custom class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All PyTorch `Modules` (layers or entire architectures) are implemented as classes with two specific methods: `__init__` and `forward`. Below is an implementation of the same archietcture as above, defining a custom class with the appropriate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the source code of a few PyTorch modules and see how they implement the required methods for example the source code of the [`Linear` layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5 - Acess the weights and biases of each layer, and print their sizes.**\n",
    "You can use the PyTorch documentation for the `Linear` layer to understand how to access them.\n",
    "\n",
    "_Solution :_ `solutions/01_pytorch_q05.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NNcustom+'></a>\n",
    "## 1.4 - Definition using a custom class without relying on PyTorch modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible and rather easy in PyTorch to define a neural network without even using existing modules for the layers (modules such as `nn.Linear` above). This flexibility is particularly useful when one wants to experiment with new types of layers, or more generally an architecture that can't be written with the existing modules. It is one of the reasons why PyTorch is appreciated among researchers. However, defining a neural network in this way requires one to define the learnable parameters, intialize them, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6 - Bonus question: Implement the same feedforward neural network as above, but this time without using the `nn.Linear` module**.\n",
    "\n",
    "**Hints -** Use:\n",
    "- `torch.empty` to create parameters\n",
    "- `kaiming_uniform_` to initialize weight matrices\n",
    "- `uniform_` to initialize vectors\n",
    "- `torch.nn.Parameter` to make parameters learnable\n",
    "- `matmul`, `sigmoid` and `softmax` for the foward pass\n",
    "\n",
    "_Solution :_ `solutions/01_pytorch_q06.py`.\n",
    "\n",
    "_Note: this is an exercice, i.e, this network can very well be implemented with `nn.Linear`, but we implement it without `nn.Linear` for training purposes. Of course, in real life, you should use existing layers when nothing prevents you from doing so._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_network(nn.Module):\n",
    "    def __init__(self, d_in, d_h1, d_out):\n",
    "        super().__init__()\n",
    "        # Create the parameters of the network\n",
    "        W_hidden = ...\n",
    "        b_hidden = ...\n",
    "        W_output = ...\n",
    "        b_output = ...\n",
    "        \n",
    "        # Initialize the parameters with nn.init.kaiming_uniform_\n",
    "        # and nn.init.normal_.\n",
    "        # One could have chosen another type of initialization\n",
    "        ...\n",
    "        \n",
    "        # Make tensors learnable parameters with torch.nn.Parameter\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        x: tensor, shape (batch_size, d_in)\n",
    "        \"\"\"\n",
    "        # Compute the forward pass\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on simulated data below that the forward pass works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.zeros(784)\n",
    "Sigma = np.eye(784)\n",
    "X = torch.tensor(np.random.multivariate_normal(mu, Sigma, size=10), dtype=torch.float)\n",
    "model = My_network(d_in=784, d_h1=256, d_out=10)\n",
    "pred = model(X)\n",
    "print(pred[0])\n",
    "\n",
    "# Also check that you can compute the gradient of the model:\n",
    "torch.sum(pred).backward()\n",
    "assert next(model.parameters()).grad is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NNtraining'></a>\n",
    "# 2 - Training a neural network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have learned how to define a neural network in PyTorch, but not how to train it. Let's take the covertype data set and train a feedforward neural network on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Loaddata'></a>\n",
    "## 2.1 - Loading openml data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the covertype dataset. This dataset represents tiles from cartographic data and aims to classify them between elements with forest or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = openml.tasks.get_task(361061) \n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "    dataset_format=\"array\", target=dataset.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train/validation/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "     X, y, test_size=20000, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "     X_train_val, y_train_val, test_size=20000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (526602, 10)\n",
      "Shape of X_val: (20000, 10)\n",
      "Shape of X_test: (20000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of X_val: {X_val.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides tools to make data loading efficient and readable. In particular, it provides the `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` classes (see doc [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) ) which allow you to load the data by batch, using multiprocessing, along with useful options such as data shuffling or applying transformations to the data.\n",
    "\n",
    "Loading the data by batch is essential when large datasets (for example images datasets) just do not fit in memory, and it is practical to speed up debugging and development iterations (since one does not have to wait for data loading each time the code is launched).\n",
    "\n",
    "To be able to use the `DataLoader` utility, we will convert our data to `TensorDataset`objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float),\n",
    "    torch.tensor(y_train, dtype=torch.long)\n",
    ")\n",
    "valset = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float),\n",
    "    torch.tensor(y_val, dtype=torch.long)\n",
    ")\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float),\n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader` can then be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=20000)\n",
    "val_loader = DataLoader(valset, batch_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covertype data set is now loaded and ready to be used to train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TrainNNPytorch'></a>\n",
    "## 2.2 - The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7 - Create a neural network of your choice with the class `nn.Sequential` that is appropriate for the covertype dataset.** Store the model into a variable called `model`.\n",
    "\n",
    "You can use the `LogSoftmax` output that is compatible with the Negative Log Likelihood Loss (see documentation here https://pytorch.org/docs/stable/nn.html#nllloss)\n",
    "\n",
    "_Solution:_ `solutions/01_pytorch_q07.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is derived, we have to define a loss (doc [here](https://pytorch.org/docs/stable/nn.html#loss-functions)) and an optimizer (doc [here](https://pytorch.org/docs/stable/optim.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an evaluation function that will compute the validation loss for our model.\n",
    "\n",
    "**Q8.1 - Can you explain what is the purpose of `torch.no_grad()` and `model.eval/train`.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    n_val = len(val_loader)\n",
    "\n",
    "    loss_val = 0\n",
    "    accuracy = 0\n",
    "    # Compute validation loss\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for xb_val, yb_val in val_loader:\n",
    "            prob_val = model(xb_val)\n",
    "            loss = criterion(prob_val, yb_val)\n",
    "            loss_val += loss.item()\n",
    "\n",
    "            y_pred = torch.argmax(prob_val, dim=1)\n",
    "            accuracy += (y_pred == yb_val).to(float).mean().item()\n",
    "        model.train()\n",
    "\n",
    "    val_loss = loss_val / n_val\n",
    "    val_accuracy = accuracy / n_val\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8 - Train the network you have created during a small number of epochs (for example 10).**\n",
    "\n",
    "**Hint -** See [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network) in the PyTorch Blitz tutorial for an example of how to train a network.\n",
    "\n",
    "_Solution :_ `solutions/01_pytorch_q08.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, n_epochs=10, verbose=True):\n",
    "\n",
    "    # Loop over epochs\n",
    "    for e in range(n_epochs):\n",
    "\n",
    "        # Perform the training steps with the SGD optimizer\n",
    "        ...\n",
    "\n",
    "        val_loss, val_accuracy = evaluate_model(model)\n",
    "        if verbose:\n",
    "            print(\"Epoch number\", e+1)\n",
    "            print(\"------------\")\n",
    "            print(f\"Training loss: {training_loss}\")\n",
    "            print(f\"Val loss: {val_loss}\")\n",
    "            print(f\"Val Accuracy: {val_accuracy}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand more about what happens when calling when using `loss.backward()` and `optimizer.step()`, the following cell prints a few elements in between the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), 0.01)\n",
    "epochs = 1\n",
    "\n",
    "u=0\n",
    "for xb, yb in trainloader:\n",
    "    while u <2:\n",
    "        print('\\n Minibatch number', u,'\\n')\n",
    "\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        prob = model(xb)\n",
    "        \n",
    "        print('The output probabilities  for the first 2 samples are', '\\n', torch.exp(prob[0:2]))\n",
    "        print('The labels  for the first 2 samples are', yb[0:2],'\\n' )\n",
    "        \n",
    "        loss = criterion(prob, yb)\n",
    "              \n",
    "        print('A few coefs of the model parameters are \\n',  next(model.parameters())[0:3, 0:3],'\\n')\n",
    "        print('Before backward step, the gradient of the loss wrt these few coefs of the model parameters are\\n', next(model.parameters()).grad ,'\\n')\n",
    "\n",
    "        loss.backward()\n",
    "        print('After backward step, the gradient of the loss wrt these few coefs of the model parameters are\\n', next(model.parameters()).grad[0:3,0:3],'\\n')\n",
    "        optimizer.step()\n",
    "        print('After optimizer step, these coefs of the model parameters are\\n', next(model.parameters())[0:3,0:3],'\\n')\n",
    "        \n",
    "        u += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wandb'></a>\n",
    "# 3 - Using weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight and Biases is a platform that eases model development. Today we will focus on:\n",
    "* experiment tracking: it allows to track and visualize various metrics in real-time (for example the training and validation loss).\n",
    "* Hyperparameter optimization: it is key to the performance of deep learning models, but can be computation intensive and painful. WandB provides an interface to perform parameter sweeps and visualize the effect of hyperparameters easily.\n",
    "\n",
    "WandB is free, but you will need to create an account to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9 - Following the instructions provided [here](https://docs.wandb.ai/quickstart), modify your code above to record and visualize in WandB the following metrics: training loss, validation loss and validation accuracy.**\n",
    "\n",
    "_Solution :_ `solutions/01_pytorch_q09.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "n_epochs = 10\n",
    "weight_decay = 0\n",
    "config = {\n",
    "  \"lr\": lr,\n",
    "  \"n_epochs\": n_epochs,\n",
    "  \"weight_decay\": weight_decay\n",
    "}\n",
    "\n",
    "# Init a Wandb run. The logs will be recrder in the MLP_covertype project as \n",
    "# run `MLP_covertype1`, and asssoicated with the hyperparameters in `config`.\n",
    "wandb.init(project=\"MLP_covertype\", name=\"MLP_covertype1\", config=config)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr=wandb.config.lr, weight_decay=wandb.config.weight_decay)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "train(model, optimizer, criterion, verbose=False, use_wandb=True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10 - Following the instructions provided [here](https://docs.wandb.ai/ref/python/sweep#examples), lauch a sweep (hypermarameter search) and visualize the results.**\n",
    "\n",
    "_Note_ consider the following sweep parameters:\n",
    "\n",
    "```\n",
    "'parameters': {\n",
    "    'lr': {'max': 0.1, 'min': 0.0001},\n",
    "    'weight_decay': { \"values\": [0, 1e-5, 1e-4]}\n",
    "}\n",
    "```\n",
    "\n",
    "_Solution :_ `solutions/01_pytorch_q10.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define training function that takes in hyperparameter \n",
    "# values from `wandb.config` and uses them to train a model and return metric\n",
    "def main():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define sweep config\n",
    "sweep_configuration = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize sweep by passing in config and start the sweep with `wandb.agent`\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resnet'></a>\n",
    "# 4 - Implementing a Resnet for tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Resnet architecture presented [Revisiting Deep Learning Models for Tabular Data](https://arxiv.org/pdf/2106.11959.p) is one of the state-of-the-art architectures for tabular data. Its architecture is described in paragraph 3.2 of the paper.\n",
    "\n",
    "**Q11 - Implement the Resnet architecture and train it on the covertype data.**\n",
    "\n",
    "Hints:\n",
    "* You may want to code first the ResNetBlock, then check that the forward pass works, before using it to build the complete RestNet architecture.\n",
    "* To start with, do not hesitate to code a simplified version of the network (for example without BatchNorm, Dropout, ...). Add components little by little one your simplified version works.\n",
    "* We are in a classification setting, do not forget the Softmax!\n",
    "* Choose the size of the hidden layers as you like. Note however that the ResNeBlock needs to output a vector the same size as your input.\n",
    "\n",
    "_Solution :_ `solutions/01_pytorch_q11.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the forward pass of the entire ResNet\n",
    "resnet = ResNet(d=X.shape[1], d_out=2, dropout_rate=0.5, n_resnet_blocks=2)\n",
    "pred = resnet(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=0.01)\n",
    "train(resnet, optimizer, criterion, n_epochs=3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
